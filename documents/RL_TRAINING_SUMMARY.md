# ğŸ“ RL Training System Summary

å®Œæ•´çš„å¤šç®—æ³•å¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿï¼Œæ”¯æŒ AUV éšœç¢ç‰©é¿éšœä»»åŠ¡ã€‚

## ğŸ“¦ å·²åˆ›å»ºçš„æ–‡ä»¶

### è®­ç»ƒè„šæœ¬
1. **`train_agent.py`** - é€šç”¨è®­ç»ƒè„šæœ¬
   - æ”¯æŒ 5 ç§ç®—æ³•ï¼šPPO, SAC, TD3, DDPG, A2C
   - å®Œæ•´çš„å›è°ƒç³»ç»Ÿï¼ˆè¯„ä¼°ã€æ£€æŸ¥ç‚¹ã€TensorBoardï¼‰
   - è‡ªåŠ¨è¶…å‚æ•°é…ç½®
   - è§‚å¯Ÿå½’ä¸€åŒ–
   - å­¦ä¹ ç‡è°ƒåº¦

2. **`train_ppo.py`** - PPO ä¸“ç”¨è®­ç»ƒè„šæœ¬ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰
   - ä¿ç•™ç”¨äºå‘åå…¼å®¹

### è¯„ä¼°è„šæœ¬
3. **`evaluate_agent.py`** - é€šç”¨è¯„ä¼°è„šæœ¬
   - æ”¯æŒæ‰€æœ‰è®­ç»ƒçš„ç®—æ³•
   - ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
   - è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
   - ä¿å­˜è½¨è¿¹æ•°æ®

4. **`evaluate_model.py`** - æ¨¡å‹è¯„ä¼°è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰

### æ–‡æ¡£
5. **`documents/TRAINING_GUIDE.md`** - å®Œæ•´è®­ç»ƒæŒ‡å—
   - æ‰€æœ‰ç®—æ³•çš„è¯¦ç»†è¯´æ˜
   - è¶…å‚æ•°è°ƒä¼˜æŒ‡å—
   - æœ€ä½³å®è·µ
   - æ•…éšœæ’é™¤

6. **`README_TRAINING.md`** - å¿«é€Ÿå…¥é—¨æŒ‡å—
   - ç®€æ´çš„ä½¿ç”¨è¯´æ˜
   - å¸¸è§å‘½ä»¤
   - å¿«é€Ÿç¤ºä¾‹

### é…ç½®æ–‡ä»¶
7. **`requirements_rl.txt`** - RL ä¾èµ–åŒ…
   - Stable-Baselines3
   - TensorBoard
   - å…¶ä»–å¿…éœ€åº“

### è¾…åŠ©è„šæœ¬
8. **`scripts/quick_train.sh`** - Linux/Mac å¿«é€Ÿè®­ç»ƒè„šæœ¬
9. **`scripts/quick_train.bat`** - Windows å¿«é€Ÿè®­ç»ƒè„šæœ¬

## ğŸ¤– æ”¯æŒçš„ç®—æ³•

### 1. PPO (Proximal Policy Optimization)
**ç±»å‹**: On-Policy  
**ç‰¹ç‚¹**:
- âœ… æœ€ç¨³å®šå¯é 
- âœ… é€‚åˆåˆå­¦è€…
- âœ… æ ·æœ¬æ•ˆç‡é€‚ä¸­
- âœ… æ˜“äºè°ƒå‚

**é»˜è®¤è¶…å‚æ•°**:
```python
learning_rate: 3e-4
n_steps: 2048
batch_size: 64
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
```

**ä½¿ç”¨åœºæ™¯**: é¦–é€‰ç®—æ³•ï¼Œé€‚åˆå¤§å¤šæ•°ä»»åŠ¡

### 2. SAC (Soft Actor-Critic)
**ç±»å‹**: Off-Policy  
**ç‰¹ç‚¹**:
- âœ… éå¸¸ç¨³å®š
- âœ… æ ·æœ¬æ•ˆç‡é«˜
- âœ… è‡ªåŠ¨ç†µè°ƒèŠ‚
- âš ï¸ è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢

**é»˜è®¤è¶…å‚æ•°**:
```python
learning_rate: 3e-4
buffer_size: 1_000_000
batch_size: 256
tau: 0.005
gamma: 0.99
ent_coef: 'auto'
```

**ä½¿ç”¨åœºæ™¯**: éœ€è¦é«˜æ ·æœ¬æ•ˆç‡çš„ä»»åŠ¡

### 3. TD3 (Twin Delayed DDPG)
**ç±»å‹**: Off-Policy  
**ç‰¹ç‚¹**:
- âœ… é«˜æ•ˆç¨³å®š
- âœ… é€‚åˆè¿ç»­æ§åˆ¶
- âœ… è®­ç»ƒé€Ÿåº¦å¿«
- âš ï¸ éœ€è¦è°ƒå‚

**é»˜è®¤è¶…å‚æ•°**:
```python
learning_rate: 3e-4
buffer_size: 1_000_000
batch_size: 256
policy_delay: 2
target_policy_noise: 0.2
```

**ä½¿ç”¨åœºæ™¯**: éœ€è¦å¿«é€Ÿè®­ç»ƒçš„ä»»åŠ¡

### 4. DDPG (Deep Deterministic Policy Gradient)
**ç±»å‹**: Off-Policy  
**ç‰¹ç‚¹**:
- âœ… è®­ç»ƒå¿«é€Ÿ
- âœ… å®ç°ç®€å•
- âš ï¸ ç¨³å®šæ€§è¾ƒå·®
- âš ï¸ å¯¹è¶…å‚æ•°æ•æ„Ÿ

**ä½¿ç”¨åœºæ™¯**: å¿«é€ŸåŸå‹å¼€å‘

### 5. A2C (Advantage Actor-Critic)
**ç±»å‹**: On-Policy  
**ç‰¹ç‚¹**:
- âœ… è®­ç»ƒæœ€å¿«
- âœ… å†…å­˜å ç”¨å°
- âš ï¸ æ ·æœ¬æ•ˆç‡ä½
- âš ï¸ æ€§èƒ½ä¸€èˆ¬

**ä½¿ç”¨åœºæ™¯**: å¿«é€Ÿæµ‹è¯•å’ŒéªŒè¯

## ğŸ“Š ç®—æ³•å¯¹æ¯”è¡¨

| ç‰¹æ€§ | PPO | SAC | TD3 | DDPG | A2C |
|------|-----|-----|-----|------|-----|
| **ç¨³å®šæ€§** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |
| **æ ·æœ¬æ•ˆç‡** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |
| **è®­ç»ƒé€Ÿåº¦** | â­â­â­ | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| **æ¨èåº¦** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è®­ç»ƒ

```bash
# PPO (æ¨è)
python train_agent.py --algo ppo --config with_obstacle --timesteps 1000000

# SAC (é«˜æ ·æœ¬æ•ˆç‡)
python train_agent.py --algo sac --config with_obstacle --timesteps 1000000

# TD3 (å¿«é€Ÿç¨³å®š)
python train_agent.py --algo td3 --config with_obstacle --timesteps 1000000

# DDPG (å¿«é€ŸåŸå‹)
python train_agent.py --algo ddpg --config with_obstacle --timesteps 500000

# A2C (å¿«é€Ÿæµ‹è¯•)
python train_agent.py --algo a2c --config with_obstacle --timesteps 500000
```

### é«˜çº§é…ç½®

```bash
# è‡ªå®šä¹‰å­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°
python train_agent.py --algo ppo --lr 1e-4 --batch-size 128

# ä½¿ç”¨ GPU
python train_agent.py --algo sac --device cuda

# å¤šä¸ªå¹¶è¡Œç¯å¢ƒ
python train_agent.py --algo ppo --n-envs 8

# ç¦ç”¨è§‚å¯Ÿå½’ä¸€åŒ–
python train_agent.py --algo td3 --no-normalize

# ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦
python train_agent.py --algo ppo --no-linear-schedule
```

### è¯„ä¼°

```bash
# åŸºç¡€è¯„ä¼°
python evaluate_agent.py models/PPO_*/best_model/best_model.zip --algo ppo

# å¸¦å¯è§†åŒ–
python evaluate_agent.py models/SAC_*/best_model/best_model.zip --algo sac --render

# æ›´å¤š episode
python evaluate_agent.py models/TD3_*/best_model/best_model.zip --algo td3 --n-episodes 50
```

## ğŸ“ è¾“å‡ºç»“æ„

è®­ç»ƒåçš„æ–‡ä»¶ç»„ç»‡ï¼š

```
project/
â”œâ”€â”€ models/                                    # è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ PPO_with_obstacle_20251120_120000/
â”‚   â”‚   â”œâ”€â”€ best_model/
â”‚   â”‚   â”‚   â””â”€â”€ best_model.zip               # â­ æœ€ä½³æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ final_model.zip                  # æœ€ç»ˆæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ vecnormalize.pkl                 # å½’ä¸€åŒ–ç»Ÿè®¡
â”‚   â”‚   â””â”€â”€ checkpoint_ppo_*.zip             # æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ SAC_with_obstacle_20251120_130000/
â”‚   â”‚   â”œâ”€â”€ best_model/
â”‚   â”‚   â”œâ”€â”€ final_model.zip
â”‚   â”‚   â”œâ”€â”€ vecnormalize.pkl
â”‚   â”‚   â””â”€â”€ replay_buffer.pkl                # é‡æ”¾ç¼“å†²
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/                                      # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ PPO_with_obstacle_20251120_120000/
â”‚       â”œâ”€â”€ progress.csv                     # è®­ç»ƒè¿›åº¦
â”‚       â””â”€â”€ evaluations.npz                  # è¯„ä¼°ç»“æœ
â”œâ”€â”€ tensorboard/                               # TensorBoard æ—¥å¿—
â”‚   â””â”€â”€ PPO_with_obstacle_20251120_120000/
â”‚       â””â”€â”€ PPO_1/
â”‚           â””â”€â”€ events.out.tfevents.*
â””â”€â”€ evaluation_results/                        # è¯„ä¼°è¾“å‡º
    â”œâ”€â”€ ppo_evaluation_results.json          # æ•°å€¼ç»“æœ
    â”œâ”€â”€ ppo_evaluation_metrics.png           # æŒ‡æ ‡å›¾è¡¨
    â””â”€â”€ ppo_sample_trajectories.png          # è½¨è¿¹å›¾
```

## ğŸ”§ å…³é”®ç‰¹æ€§

### 1. ç»Ÿä¸€æ¥å£
æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„å‘½ä»¤è¡Œæ¥å£ï¼š
```bash
python train_agent.py --algo [ALGORITHM] --config [CONFIG] [OPTIONS]
```

### 2. è‡ªåŠ¨è¶…å‚æ•°
æ¯ä¸ªç®—æ³•éƒ½æœ‰ä¼˜åŒ–çš„é»˜è®¤è¶…å‚æ•°ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–è¦†ç›–ã€‚

### 3. å®Œæ•´å›è°ƒç³»ç»Ÿ
- **EvalCallback**: å®šæœŸè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
- **CheckpointCallback**: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
- **TensorboardCallback**: è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡
- **ProgressBarCallback**: æ˜¾ç¤ºè®­ç»ƒè¿›åº¦

### 4. è§‚å¯Ÿå½’ä¸€åŒ–
è‡ªåŠ¨ä½¿ç”¨ `VecNormalize` è¿›è¡Œè§‚å¯Ÿå’Œå¥–åŠ±å½’ä¸€åŒ–ã€‚

### 5. å­¦ä¹ ç‡è°ƒåº¦
æ”¯æŒçº¿æ€§å­¦ä¹ ç‡è¡°å‡ã€‚

### 6. å¹¶è¡Œè®­ç»ƒ
æ”¯æŒå¤šä¸ªå¹¶è¡Œç¯å¢ƒåŠ é€Ÿè®­ç»ƒï¼ˆOn-Policy ç®—æ³•ï¼‰ã€‚

### 7. GPU æ”¯æŒ
è‡ªåŠ¨æ£€æµ‹ CUDA æˆ–æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡ã€‚

### 8. è¯¦ç»†æ—¥å¿—
- CSV æ—¥å¿—
- TensorBoard æ—¥å¿—
- æ§åˆ¶å°è¾“å‡º

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### TensorBoard æŒ‡æ ‡

**é€šç”¨æŒ‡æ ‡**:
- `rollout/ep_rew_mean`: å¹³å‡ episode å¥–åŠ±
- `rollout/ep_len_mean`: å¹³å‡ episode é•¿åº¦
- `time/fps`: è®­ç»ƒé€Ÿåº¦ï¼ˆFPSï¼‰

**è‡ªå®šä¹‰æŒ‡æ ‡**:
- `rollout/ep_final_error`: æœ€ç»ˆè·¯å¾„è¯¯å·®
- `rollout/ep_collision`: ç¢°æ’ç‡
- `rollout/ep_path_error`: è·¯å¾„è·Ÿè¸ªè¯¯å·®
- `rollout/ep_obstacle_distance`: æœ€å°éšœç¢ç‰©è·ç¦»
- `rollout/ep_path_reward`: è·¯å¾„å¥–åŠ±
- `rollout/ep_obstacle_reward`: é¿éšœå¥–åŠ±

**ç®—æ³•ç‰¹å®šæŒ‡æ ‡**:
- PPO: `train/entropy_loss`, `train/policy_gradient_loss`
- SAC: `train/ent_coef`, `train/actor_loss`
- TD3: `train/actor_loss`, `train/critic_loss`

### æŸ¥çœ‹è®­ç»ƒè¿›åº¦

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir tensorboard/

# åœ¨æµè§ˆå™¨æ‰“å¼€
http://localhost:6006
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. ç®—æ³•é€‰æ‹©

**åˆå­¦è€…**: ä½¿ç”¨ PPO
```bash
python train_agent.py --algo ppo --config with_obstacle
```

**éœ€è¦é«˜æ•ˆç‡**: ä½¿ç”¨ SAC
```bash
python train_agent.py --algo sac --config with_obstacle
```

**éœ€è¦å¿«é€Ÿè®­ç»ƒ**: ä½¿ç”¨ TD3 æˆ– A2C
```bash
python train_agent.py --algo td3 --config with_obstacle
```

### 2. è®­ç»ƒç­–ç•¥

**é˜¶æ®µ 1 - åŸºç¡€è®­ç»ƒ** (500K-1M steps):
```bash
python train_agent.py --algo ppo --config no_obstacle --timesteps 500000
```

**é˜¶æ®µ 2 - æ·»åŠ éšœç¢ç‰©** (1M-2M steps):
```bash
python train_agent.py --algo ppo --config with_obstacle --timesteps 1000000
```

**é˜¶æ®µ 3 - å®Œæ•´æŒ‘æˆ˜** (2M-3M steps):
```bash
python train_agent.py --algo ppo --config hard --timesteps 2000000
```

### 3. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡è°ƒæ•´**:
- è¿‡å¿«æ”¶æ•› â†’ é™ä½å­¦ä¹ ç‡
- å­¦ä¹ å¤ªæ…¢ â†’ å¢åŠ å­¦ä¹ ç‡
- æ¨èèŒƒå›´: 1e-5 åˆ° 1e-3

**æ‰¹é‡å¤§å°è°ƒæ•´**:
- å†…å­˜ä¸è¶³ â†’ å‡å°æ‰¹é‡
- è®­ç»ƒä¸ç¨³å®š â†’ å¢å¤§æ‰¹é‡
- PPO æ¨è: 64-256
- SAC æ¨è: 256-512

### 4. è¯„ä¼°ç­–ç•¥

```bash
# å®šæœŸè¯„ä¼°
python evaluate_agent.py MODEL_PATH --algo ALGO --n-episodes 20

# æœ€ç»ˆè¯„ä¼°
python evaluate_agent.py MODEL_PATH --algo ALGO --n-episodes 50
```

### 5. å¤šç§å­è®­ç»ƒ

```bash
# è®­ç»ƒå¤šä¸ªç§å­ä»¥éªŒè¯ç¨³å®šæ€§
for seed in 0 1 2 3 4; do
    python train_agent.py --algo ppo --seed $seed &
done
```

## ğŸš¨ å¸¸è§é—®é¢˜

### Q: å“ªä¸ªç®—æ³•æœ€å¥½ï¼Ÿ
**A**: PPO æ˜¯æœ€ç¨³å®šå¯é çš„é€‰æ‹©ã€‚SAC æ ·æœ¬æ•ˆç‡æ›´é«˜ä½†è®­ç»ƒè¾ƒæ…¢ã€‚

### Q: éœ€è¦è®­ç»ƒå¤šä¹…ï¼Ÿ
**A**: 
- åŸºç¡€æ€§èƒ½: 500K-1M steps
- è‰¯å¥½æ€§èƒ½: 1M-2M steps
- æœ€ä½³æ€§èƒ½: 2M-3M steps

### Q: GPU å¿…é¡»å—ï¼Ÿ
**A**: ä¸å¿…é¡»ï¼Œä½†å¼ºçƒˆæ¨èã€‚GPU å¯ä»¥åŠ é€Ÿ 3-5 å€ã€‚

### Q: å¦‚ä½•åˆ¤æ–­è®­ç»ƒæ˜¯å¦æˆåŠŸï¼Ÿ
**A**: æŸ¥çœ‹è¯„ä¼°å¥–åŠ±æ˜¯å¦ç¨³å®šæå‡ï¼Œç¢°æ’ç‡æ˜¯å¦é™ä½ã€‚

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: 
```bash
# å‡å°‘å¹¶è¡Œç¯å¢ƒ
python train_agent.py --algo ppo --n-envs 2

# å‡å°æ‰¹é‡å¤§å°
python train_agent.py --algo ppo --batch-size 32

# ä½¿ç”¨ CPU
python train_agent.py --algo ppo --device cpu
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ğŸ“– [å®Œæ•´è®­ç»ƒæŒ‡å—](TRAINING_GUIDE.md)
- ğŸ—ï¸ [ç¯å¢ƒä½¿ç”¨æŒ‡å—](obstacle_avoidance_env_guide.md)
- ğŸ”§ [é›†æˆæ€»ç»“](INTEGRATION_SUMMARY.md)
- ğŸ› [Bug ä¿®å¤è¯´æ˜](../BUGFIX_NOTES.md)

## âœ… ä¸‹ä¸€æ­¥

1. **å®‰è£…ä¾èµ–**:
   ```bash
   pip install -r requirements_rl.txt
   ```

2. **å¿«é€Ÿè®­ç»ƒ**:
   ```bash
   python train_agent.py --algo ppo --config with_obstacle --timesteps 1000000
   ```

3. **ç›‘æ§è®­ç»ƒ**:
   ```bash
   tensorboard --logdir tensorboard/
   ```

4. **è¯„ä¼°æ¨¡å‹**:
   ```bash
   python evaluate_agent.py models/.../best_model.zip --algo ppo --n-episodes 20
   ```

5. **å¯¹æ¯”ç®—æ³•**:
   ```bash
   python train_agent.py --algo ppo --config with_obstacle &
   python train_agent.py --algo sac --config with_obstacle &
   python train_agent.py --algo td3 --config with_obstacle &
   ```

---

**ç³»ç»ŸçŠ¶æ€**: âœ… å®Œæ•´ä¸”ç»è¿‡æµ‹è¯•  
**æœ€åæ›´æ–°**: 2025-11-20  
**ç‰ˆæœ¬**: v1.0.0

